\section{Optimal Control of Pitch/Travel with Feedback (LQ)}\label{sec:prob3}

\subsection{Discrete LQR}

To eliminate the discrepency between the optimal and measured trajectory observed in figure \ref{fig:opt_traj}, we can update the optimal trajectory for every time step with a state feedback term weighted by a suitable gain matrix:
\begin{equation*}
u_k = u_k^* - K^\top(x_k - x_k^*),
\end{equation*}

or, alternatively
\begin{equation}
\label{eq:LQ_ctrl}
\Delta u_k = - K^\top \Delta x_k,
\end{equation}

where 
\begin{align*}
\Delta x_k &= x_k - x_k^*,\\
\Delta u_k &= u_k - u_k^*.
\end{align*}

It can be shown (some clever reference here...) that the controller \eqref{eq:LQ_ctrl} is the optimal solution which minimizes the quadratic objective function

\begin{equation*}
	J = \sum_{i=0}^{\infty} \Delta x_{i+1}^\top Q \Delta x_{i+1} + \Delta u_i^\top R \Delta u_i,
\end{equation*}

subject to the system dynamics \eqref{eq:dmodel}, where
\begin{equation}
\label{eq:LQ_gain}
	K = (R + B^\top P B)^{-1} B^\top P A,
\end{equation}

and $P$ is the unique positive definite solution to the discrete time algebraic Riccati equation. \eqref{eq:LQ_gain} is used as the state feedback gain, and the resulting Linear Quadratic controller is implemented, with weigthing matrices $Q$ and $R$ chosen to penalize deviations in states and input for a satisfactory results.

Maybe discuss (L)QR (pun intended), tuning...


\subsection{Results and discussion}

\begin{figure}[hp]
	\centering
		\includegraphics[width=1.00\textwidth]{figures/3/lqr.eps}
	\caption{LQR}
	\label{fig:lqr}
\end{figure}

\subsubsection{MPC discussion}

